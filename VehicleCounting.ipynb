{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a83f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imutils\n",
    "import time\n",
    "from scipy import spatial\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3494187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these classes will be counted as 'vehicles'\n",
    "list_of_vehicles = [\"bicycle\",\"car\",\"motorbike\",\"bus\",\"truck\"]\n",
    "# Setting the threshold for the number of frames to search a vehicle for\n",
    "FRAMES_BEFORE_CURRENT = 10 \n",
    "inputWidth, inputHeight = 416, 416\n",
    "#Assign file paths\n",
    "inputVideoPath=os.path.sep.join([\"./videos/cars.mp4\"])\n",
    "labelsPath= os.path.sep.join([\"./yolo-coco/coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "weightsPath = os.path.sep.join([\"./yolo-coco/yolov3.weights\"])\n",
    "configPath = os.path.sep.join([\"./yolo-coco/yolov3.cfg\"])\n",
    "outputVideoPath = os.path.sep.join([\"./outputVideos/VehicleCounting_Output.avi\"])\n",
    "\n",
    "USE_GPU=0\n",
    "\n",
    "# Initialize a list of colors to represent each possible class label\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
    "\tdtype=\"uint8\")\n",
    "# PURPOSE: Displays the vehicle count on the top-left corner of the frame\n",
    "# PARAMETERS: Frame on which the count is displayed, the count number of vehicles \n",
    "# RETURN: N/A\n",
    "def displayVehicleCount(frame, vehicle_count):\n",
    "\tcv2.putText(\n",
    "\t\tframe, #Image\n",
    "\t\t'Number of vehicles in the restricted area: ' + str(vehicle_count), #Label\n",
    "\t\t(20, 50), #Position\n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, #Font\n",
    "\t\t1, #Size\n",
    "\t\t(0, 0, 0xFF), #Color\n",
    "\t\t2, #Thickness\n",
    "\t\tcv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "\t\t)\n",
    "\n",
    "# PURPOSE: Determining if the box-mid point cross the red line or are within the range of 5 units\n",
    "# from the line\n",
    "# PARAMETERS: X Mid-Point of the box, Y mid-point of the box, Coordinates of the line \n",
    "# RETURN: \n",
    "# - True if the midpoint of the box overlaps with the line within a threshold of 5 units \n",
    "# - False if the midpoint of the box lies outside the line and threshold\n",
    "def boxAndLineOverlap(x_mid_point, y_mid_point, line_coordinates):\n",
    "\n",
    "\tx1_line, y1_line, x2_line, y2_line = line_coordinates #Unpacking\n",
    "\n",
    "\tif (x_mid_point >= x1_line and x_mid_point <= x2_line+5) and\\\n",
    "\t\t(y_mid_point >= y1_line and y_mid_point <= y2_line+5):\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "# PURPOSE: Determining if the box-mid point cross the orange line or are within the range of 5 units\n",
    "# from the line\n",
    "# PARAMETERS: X Mid-Point of the box, Y mid-point of the box, Coordinates of the line \n",
    "# RETURN: \n",
    "# - True if the midpoint of the box overlaps with the line within a threshold of 5 units \n",
    "# - False if the midpoint of the box lies outside the line and threshold\n",
    "def boxAndLineOverlap2(x_mid_point2, y_mid_point2, line_coordinates2):\n",
    "\n",
    "\tx1_line2, y1_line2, x2_line2, y2_line2 = line_coordinates2 #Unpacking\n",
    "\n",
    "\tif (x_mid_point2 >= x1_line2 and x_mid_point2 <= x2_line2+5) and\\\n",
    "\t\t(y_mid_point2 >= y1_line2-10 and y_mid_point2 <= y2_line2+5):\n",
    "\t\treturn True\n",
    "\treturn False\n",
    "\n",
    "\n",
    "# PURPOSE: Displaying the FPS of the detected video\n",
    "# PARAMETERS: Start time of the frame, number of frames within the same second\n",
    "# RETURN: New start time, new number of frames \n",
    "def displayFPS(start_time, num_frames):\n",
    "\tcurrent_time = int(time.time())\n",
    "\tif(current_time > start_time):\n",
    "\t\tos.system('clear') # Equivalent of CTRL+L on the terminal\n",
    "\t\tprint(\"FPS:\", num_frames)\n",
    "\t\tnum_frames = 0\n",
    "\t\tstart_time = current_time\n",
    "\treturn start_time, num_frames\n",
    "\n",
    "# PURPOSE: Draw all the detection boxes with a green dot at the center\n",
    "# RETURN: N/A\n",
    "def drawDetectionBoxes(idxs, boxes, classIDs, confidences, frame):\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indices we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "\t\t\t# draw a bounding box rectangle and label on the frame\n",
    "\t\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\t\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\t\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]],\n",
    "\t\t\t\tconfidences[i])\n",
    "\t\t\tcv2.putText(frame, text, (x, y - 5),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\t\t\t#Draw a green dot in the middle of the box\n",
    "\t\t\tcv2.circle(frame, (x + (w//2), y+ (h//2)), 2, (0, 0xFF, 0), thickness=2)\n",
    "\n",
    "# PURPOSE: Initializing the video writer with the output video path and the same number\n",
    "# of fps, width and height as the source video \n",
    "# PARAMETERS: Width of the source video, Height of the source video, the video stream\n",
    "# RETURN: The initialized video writer\n",
    "def initializeVideoWriter(video_width, video_height, videoStream):\n",
    "\t# Getting the fps of the source video\n",
    "\tsourceVideofps = videoStream.get(cv2.CAP_PROP_FPS)\n",
    "\t# initialize our video writer\n",
    "\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\treturn cv2.VideoWriter(outputVideoPath, fourcc, sourceVideofps,\n",
    "\t\t(video_width, video_height), True)\n",
    "\n",
    "# PURPOSE: Identifying if the current box was present in the previous frames\n",
    "# PARAMETERS: All the vehicular detections of the previous frames, \n",
    "#\t\t\tthe coordinates of the box of previous detections\n",
    "# RETURN: True if the box was current box was present in the previous frames;\n",
    "#\t\t  False if the box was not present in the previous frames\n",
    "def boxInPreviousFrames(previous_frame_detections, current_box, current_detections):\n",
    "\tcenterX, centerY, width, height = current_box\n",
    "\tdist = np.inf #Initializing the minimum distance\n",
    "\t# Iterating through all the k-dimensional trees\n",
    "\tfor i in range(FRAMES_BEFORE_CURRENT):\n",
    "\t\tcoordinate_list = list(previous_frame_detections[i].keys())\n",
    "\t\tif len(coordinate_list) == 0: # When there are no detections in the previous frame\n",
    "\t\t\tcontinue\n",
    "\t\t# Finding the distance to the closest point and the index\n",
    "\t\ttemp_dist, index = spatial.KDTree(coordinate_list).query([(centerX, centerY)])\n",
    "\t\tif (temp_dist < dist):\n",
    "\t\t\tdist = temp_dist\n",
    "\t\t\tframe_num = i\n",
    "\t\t\tcoord = coordinate_list[index[0]]\n",
    "\n",
    "\tif (dist > (max(width, height)/2)):\n",
    "\t\treturn False\n",
    "\n",
    "\t# Keeping the vehicle ID constant\n",
    "\tcurrent_detections[(centerX, centerY)] = previous_frame_detections[frame_num][coord]\n",
    "\treturn True\n",
    "\n",
    "def count_vehicles(idxs, boxes, classIDs, vehicle_count, previous_frame_detections, frame, line_coordinates, line_coordinates2):\n",
    "\tcurrent_detections = {}\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indices we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\t\t\n",
    "\t\t\tcenterX = x + (w//2)\n",
    "\t\t\tcenterY = y+ (h//2)\n",
    "\n",
    "\t\t\t# When the detection is in the list of vehicles, AND\n",
    "\t\t\t# it crosses the line AND\n",
    "\t\t\t# the ID of the detection is not present in the vehicles\n",
    "\t\t\tif (LABELS[classIDs[i]] in list_of_vehicles)and (boxAndLineOverlap(centerX, centerY, line_coordinates)):\n",
    "\t\t\t\tx1_line, y1_line, x2_line, y2_line =  line_coordinates              \n",
    "\t\t\t\tcurrent_detections[(centerX, centerY)] = vehicle_count \n",
    "\t\t\t\tif (not boxInPreviousFrames(previous_frame_detections, (centerX, centerY, w, h), current_detections))  :\n",
    "\t\t\t\t\tvehicle_count += 1\n",
    "\t\t\t\t\tcv2.line(frame, (x1_line, y1_line), (x2_line, y2_line), (0,255,0), 5)\n",
    "\t\t\t\t# else: #ID assigning\n",
    "\t\t\t\t\t#Add the current detection mid-point of box to the list of detected items\n",
    "\t\t\t\t# Get the ID corresponding to the current detection\n",
    "\n",
    "\n",
    "\t\t\tif (LABELS[classIDs[i]] in list_of_vehicles)and (boxAndLineOverlap2(centerX, centerY, line_coordinates2)):\n",
    "\t\t\t\tx1_line2, y1_line2, x2_line2, y2_line2 =  line_coordinates2              \n",
    "\t\t\t\tcurrent_detections[(centerX, centerY)] = vehicle_count \n",
    "\t\t\t\tif (not boxInPreviousFrames(previous_frame_detections, (centerX, centerY, w, h), current_detections))  :\n",
    "\t\t\t\t\tvehicle_count -= 1\n",
    "\t\t\t\t\t#Changing line color to green if a vehicle in the frame has crossed the line                    \n",
    "\t\t\t\t\tcv2.line(frame, (x1_line2, y1_line2), (x2_line2, y2_line2), (0,255,0), 5)\n",
    "\t\t\t\t# else: #ID assigning\n",
    "\t\t\t\t\t#Add the current detection mid-point of box to the list of detected items\n",
    "\t\t\t\t# Get the ID corresponding to the current detection\n",
    "\t\t\t\t#ID = current_detections.get((centerX, centerY))\n",
    "\t\t\t\t# If there are two detections having the same ID due to being too close, \n",
    "\t\t\t\t# then assign a new ID to current detection.\n",
    "\t\t\t\t#if (list(current_detections.values()).count(ID) > 1):\n",
    "\n",
    "\t\t\t\t\t#vehicle_count += 1 \n",
    "\t\t\t\t\t#current_detections[(centerX, centerY)] = vehicle_count\n",
    "\t\t\t\t\t#ID = current_detections.get((centerX, centerY))\n",
    "\t\t\t\t#Display the ID at the center of the box\n",
    "\t\t\t\t#cv2.putText(frame, str(ID), (centerX, centerY),\\\n",
    "\t\t\t\t\t#cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0,0,255], 2)                \n",
    "\n",
    "\n",
    "\treturn vehicle_count, current_detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee266601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "# and determine only the *output* layer names that we need from YOLO\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
    "\n",
    "#Using GPU if flag is passed\n",
    "if USE_GPU:\n",
    "\tnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "\tnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# initialize the video stream, pointer to output video file, and\n",
    "# frame dimensions\n",
    "videoStream = cv2.VideoCapture(inputVideoPath)\n",
    "video_width = int(videoStream.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(videoStream.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Specifying coordinates for a default lines \n",
    "x1_line = video_width//2\n",
    "y1_line = video_height//2-20\n",
    "x2_line = video_width//2+350\n",
    "y2_line = video_height//2-20\n",
    "\n",
    "x1_line2 = video_width//2\n",
    "y1_line2 = video_height//2+120\n",
    "x2_line2 = video_width\n",
    "y2_line2 = video_height//2+120\n",
    "line_coordinates = x1_line, y1_line, x2_line, y2_line\n",
    "line_coordinates2 = x1_line2, y1_line2, x2_line2, y2_line2\n",
    "#Initialization\n",
    "previous_frame_detections = [{(0,0):0} for i in range(FRAMES_BEFORE_CURRENT)]\n",
    "# previous_frame_detections = [spatial.KDTree([(0,0)])]*FRAMES_BEFORE_CURRENT # Initializing all trees\n",
    "num_frames, vehicle_count = 0, 0\n",
    "num_frames2 = 0\n",
    "writer = initializeVideoWriter(video_width, video_height, videoStream)\n",
    "start_time = int(time.time())\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "\tprint(\"================NEW FRAME================\")\n",
    "\tnum_frames+= 1\n",
    "\tnum_frames2+= 1\n",
    "\n",
    "\tprint(\"FRAME:\\t\", num_frames)\n",
    "\t# Initialization for each iteration\n",
    "\tboxes, confidences, classIDs = [], [], [] \n",
    "\t#Calculating fps each second\n",
    "\tstart_time, num_frames = displayFPS(start_time, num_frames)\n",
    "\t# read the next frame from the file\n",
    "\t(grabbed, frame) = videoStream.read()\n",
    "\n",
    "\t# if the frame was not grabbed, then we have reached the end of the stream\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\tif cv2.waitKey(1) == 27:\n",
    "\t\tbreak\n",
    "\t# construct a blob from the input frame and then perform a forward\n",
    "\t# pass of the YOLO object detector, giving us our bounding boxes\n",
    "\t# and associated probabilities\n",
    "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (inputWidth, inputHeight),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tstart = time.time()\n",
    "\tlayerOutputs = net.forward(ln)\n",
    "\tend = time.time()\n",
    "\n",
    "\t# loop over each of the layer outputs\n",
    "\tfor output in layerOutputs:\n",
    "\t\t# loop over each of the detections\n",
    "\t\tfor i, detection in enumerate(output):\n",
    "\t\t\t# extract the class ID and confidence (i.e., probability)\n",
    "\t\t\t# of the current object detection\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\t# filter out weak predictions by ensuring the detected\n",
    "\t\t\t# probability is greater than the minimum probability\n",
    "\t\t\tif confidence > 0.5:\n",
    "\t\t\t\t# scale the bounding box coordinates back relative to\n",
    "\t\t\t\t# the size of the image, keeping in mind that YOLO\n",
    "\t\t\t\t# actually returns the center (x, y)-coordinates of\n",
    "\t\t\t\t# the bounding box followed by the boxes' width and\n",
    "\t\t\t\t# height\n",
    "\t\t\t\tbox = detection[0:4] * np.array([video_width, video_height, video_width, video_height])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t# use the center (x, y)-coordinates to derive the top\n",
    "\t\t\t\t# and and left corner of the bounding box\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\t\t\t\t#find how many cars are in the restricted area in the first frame                \n",
    "\t\t\t\tif num_frames2==1:\n",
    "\t\t\t\t\tif centerY>=y1_line and centerY<=y1_line2 and centerX>=x1_line2 and centerX<=x2_line2:\n",
    "\t\t\t\t\t\tvehicle_count+=1                        \n",
    "                                    \n",
    "\t\t\t\t#Printing the info of the detection\n",
    "\t\t\t\t#print('\\nName:\\t', LABELS[classID],\n",
    "\t\t\t\t\t#'\\t|\\tBOX:\\t', x,y)\n",
    "\n",
    "\t\t\t\t# update our list of bounding box coordinates,\n",
    "\t\t\t\t# confidences, and class IDs\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\t\t\t\tclassIDs.append(classID)\n",
    "\n",
    "\n",
    "\n",
    "\t# apply non-maxima suppression to suppress weak, overlapping\n",
    "\t# bounding boxes\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5,\n",
    "\t\t0.3)\n",
    "\n",
    "\t# Draw detection box \n",
    "\tdrawDetectionBoxes(idxs, boxes, classIDs, confidences, frame)\n",
    "\tcv2.line(frame, (x1_line, y1_line), (x2_line, y2_line), (0,0,255), 4)\n",
    "\tcv2.line(frame, (x1_line2, y1_line2), (x2_line2, y2_line2), (0,127,255), 4)\n",
    "\tvehicle_count, current_detections = count_vehicles(idxs, boxes, classIDs, vehicle_count, previous_frame_detections, frame, line_coordinates, line_coordinates2)\n",
    "\n",
    "\n",
    "\t# Display Vehicle Count if a vehicle has passed the line \n",
    "\tdisplayVehicleCount(frame, vehicle_count)\n",
    "\n",
    "    # write the output frame to disk\n",
    "\twriter.write(frame)\n",
    "\n",
    "\tcv2.imshow('Frame', frame)\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\t\n",
    "\t\n",
    "\t# Updating with the current frame detections\n",
    "\tprevious_frame_detections.pop(0) #Removing the first frame from the list\n",
    "\t# previous_frame_detections.append(spatial.KDTree(current_detections))\n",
    "\tprevious_frame_detections.append(current_detections)\n",
    "\tif cv2.waitKey(1) == 27:\n",
    "\t\tbreak\n",
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "videoStream.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
